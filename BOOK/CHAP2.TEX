\chapter{Bayesian-Python}
\thispagestyle{empty}

\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
\noindent\color{blueblack}\shadowbox{
\begin{tabular}{|p{13.8cm}|}\arrayrulecolor{darkblue}\hline
\rowcolor{darkblue} \hei\textcolor{white}{学习目标与要求}\\\hline
\rowcolor{lightblue}~~~~~~~~\kai\textcolor{darkblue}{1.~~．}\\
\rowcolor{lightblue}~~~~~~~~\kai\textcolor{darkblue}{2.~~．}\\
\rowcolor{lightblue}~~~~~~~~\kai\textcolor{darkblue}{3.~~．}\\
\rowcolor{lightblue}~~~~~~~~\kai\textcolor{darkblue}{4.~~．}\\\hline
\end{tabular}}\color{black}
\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}
%\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
%\shadowbox{
%\begin{tcolorbox}[arc=0mm,colback=white,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~了解科学计算的一般过程．}\\
%\kai\textcolor{darkblue}{2.~~了解数值计算方法的研究内容和特点．}\\
%\kai\textcolor{darkblue}{3.~~理解数值计算误差的有关概念．}\\
%\kai\textcolor{darkblue}{4.~~掌握数值计算误差的控制方法．}
%\end{tcolorbox}}
%\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}

\section{Probalilities}
A common and useful conceptualization in statistics is to think that data was generated from some probability distribution with unobserved parameters.
\subsection{Gaussian Distribution}
$$
pdf(x|\mu,\sigma)= \frac{1}{\sigma \sqrt{2\pi}}\exp ^{\frac{-(x-\mu)^2}{2\sigma ^2}}
$$
\begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import seaborn as sns

mu_params = [-1, 0, 1]
sd_params = [0.5, 1, 1.5]
x = np.linspace(-7, 7, 100)
f, ax = plt.subplots(len(mu_params), len(sd_params), sharex=True, sharey=True)
for i in range(3):
	for j in range(3):
		mu = mu_params[i]
		sd = sd_params[j]
		y = stats.norm(mu, sd).pdf(x)
		ax[i, j].plot(x,y)
		ax[i, j].plot(0, 0, label="$\\mu$ = {:3.2f}\n$\\sigma$={:3.2f}".format (mu, sd), alpha=0)
		ax[i, j].legend(fontsize=12)
	ax[2,1].set_xlabel('$x$', fontsize=16)
	ax[1,0].set_ylabel('$pdf(x)$', fontsize=16)
	plt.tight_layout()
\end{lstlisting}
\begin{figure}[htb]
\center{\includegraphics[width=10cm]  {gaussian.png}}
\caption{\label{gaussion_distribution} Gaussion Distribution}
\end{figure}

\subsection{Coin-Flipping Problen}
We will answer this question in a Bayesian setting. We will need data and a probabilistic model. 


Data: we will assume that we have already tossed a coin a number of times and we have recorded the number of observed head, so the data part is done.

Model will be discussed soon.

\subsubsection{The general model}
The firs thing we will do is generalize the concept of bias. We will say that a coin with a bias of $1$ will always land heads, one with a bias of $0$ will always land tails, and one with a bias of  $0.5$ will land half of the time heads and half of the time tails. To represent the bias, we will use the parameter $\theta$, and to represent the total number of heads for an $N$ number of tosses, we will use the variable $y$. According to Bayes' theorem we have the following formula:
$$
p(\theta | y) \propto p(y | \theta) p (\theta)
$$ 
Notice that we need to specify which prior $p(\theta)$ and likelihood $p(y | \theta)$ we will use. Let's start with the likelihood.

\subsubsection{Choosing the likelihood}
Let's assume that a coin toss does not affect other tosses, that is, we are assuming coin tosses are independent of each other. Let's also assume that only two outcomes are possible, heads or tails. Given these assumptions, a good candidate for the likelihood is the binomial distribution (二项分布):
$$
p(y  | \theta) = \frac{N !}{N! (N-y)!} \theta ^{y} (1-\theta)^{N-y}
$$ 
This is a discrete distribution returning the probability of getting $y$ heads (or in general, success) out of $N$ coin tosses (or in general, trials or experiments) given a fixed value of $\theta$. The following code generates $9$ binomial distributions; each subplot has its own legend indicating the corresponding parameters:
\begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import seaborn as sns

n_params = [1,2,4]
p_params = [0.25, 0.5, 0.75]
x = np.arange(0, max(n_params) + 1)
f, ax = plt.subplots(len(n_params), len(p_params), sharex=True, sharey=True)
for i in range(3):
    for j in range(3):
        n = n_params[i]
        p = p_params[j]
        y = stats.binom(n=n, p=p).pmf(x)
        ax[i,j].vlines(x, 0, y, colors='b', lw=5)
        ax[i,j].set_ylim(0,1)
        ax[i,j].plot(0,0,label="n = {:3.2f}\np = {:3.2f}".format(n,p), alpha=0)
        ax[i,j].legend(fontsize=12)
ax[2,1].set_xlabel('$\\theta$', fontsize=14)
ax[1,0].set_ylabel('$p(y|\\theta)$', fontsize=14)
ax[0,0].set_xticks(x)
plt.savefig('binomial_distribution.png')
\end{lstlisting} 
\begin{figure}[htb]
\center{\includegraphics[width=10cm]  {binomial_distribution.png}}
\caption{\label{gaussion_distribution}  binomial\_distribution}
\end{figure}

The binomial distribution is also a reasonable choice for the likelihood. Intuitively, we can see that $\theta$ indicates how likely it is that we will obtain a head when tossing a coin, and we have observed that event $y$ times. Following the same line of reasoning we get   that $1-\theta$ is the chance of getting a tail, and that event has occurred $N-y$ times.

OK, so if we know $\theta$, the binomial distribution will tell us the expected distribution of head. The only problem is that we do not kno $\theta !$. But do not despair; in Bayesian statistics, every time we do not know the value   of a parameter, we put a prior on it, so let's move on and choose a prior.

\subsubsection{Choosing the prior}
As a prior we will use a beta distribution (贝塔分布), which is a very common distribution in Bayesian statistics and looks like this:
$$
p(\theta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta ^{\alpha-1}(1-\theta)^{\beta - 1}
$$ 
The first term is a normalization constant that ensures the distribution integrates to $1$.

\begin{lstlisting}
  import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import seaborn as sns

params = [0.5, 1, 2, 3]
x = np.linspace(0, 1, 100)
f, ax = plt.subplots(len(params), len(params), sharex=True, sharey=True)
for i in range(4):
    for j in range(4):
        a = params[i]
        b = params[j]
        y = stats.beta(a, b).pdf(x)
        ax[i,j].plot(x, y)
        ax[i,j].plot(0,0,label="$\\alpha$ = {:3.2f}\n$\\beta$={:3.2f}".format(a,b), alpha=0)
        ax[i,j].legend(fontsize=12)
ax[3,0].set_xlabel('$\\theta$', fontsize=14)
ax[0,0].set_ylabel('$p(\\theta)$', fontsize=14)
plt.savefig('beta_distribution.png')
\end{lstlisting}
\begin{figure}[htb]
\center{\includegraphics[width=10cm]  {beta_distribution.png}}
\caption{\label{beta_distribution}  beta\_distribution}
\end{figure}

Why are we using the beta distribution for our model? 1) One reason is that the beta distribution is restricted to be between $0$ and $1$, in the same way our parameter $\theta$ is. 2) Another reason is its versatility (通用性). As we can see in the preceding figure, the distribution adopts several shapes, including a uniform distribution, Gaussian-like distributions, U-like distributions, and so on. 3) A third reason is that the beta distribution is the conjugate prior (共轭先验) of the binomial distribution (which we are using as the likelihood). A conjugate prior of a likelihood is a prior that, when used in combination with the given likelihood, returns a posterior with the same functional form as the prior. There are other pairs of conjugate priors, for example, the Gaussion distribution is the conjugate prior of itself.

For many years, Bayesian analysis was restricted to the use of conjugate priors. Conjugacy ensures mathematical tractability of the posterior, which is important given that  common problem in Bayesian statics is to end up with a posterior we cannot solve analytically. This was a deal beaker before the  development of suitable computational methods to solve any possible posterior. 

However, modern computational methods to solve Bayesian problems whether we choose conjugate priors or not.

\subsubsection{Getting the posterior}
The Bayes' theorem says that the posterior is proportional to the  likelihood times the prior:
$$
p(\theta | y) \propto p(y | \theta) p (\theta)
$$ 
which turns out to be
$$
p(\theta | y) \propto \frac{N !}{N! (N-y)!} \theta ^{y} (1-\theta)^{N-y}  \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta ^{\alpha-1}(1-\theta)^{\beta - 1}
$$
To our practical concerns we can drop all the terms that do not depend on $\theta$ and our results will still be valid. So we can write the following:
$$
p(\theta | y) \propto \theta ^{y} (1-\theta)^{N-y}\theta ^{\alpha-1}(1-\theta)^{\beta - 1}
$$
Reordering it, we have
$$
p(\theta | y) \propto \theta^{\alpha-1+y}(1-\theta)^{\beta-1+N-y}
$$
We will see that this expression has the same functional form of a beta distribution (except for the normalization) with $\alpha_{posterior}=\alpha_{perior}+y$ and $\beta_{posterior}=\beta_{prior}+N-y$,  which means that the posterior for our problem is the beta distribution:
$$
p(\theta | y) = Beta(\alpha_{prior} + y, \beta_{prior}+N-y)
$$ 
\begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import seaborn as sns

theta_real = 0.35
trials = [0, 1, 2, 3, 4, 8, 16, 32, 50, 150]
data = [0, 1, 1, 1, 1, 4, 6, 9, 13, 48]

beta_params = [(1,1), (0.5,0.5), (20,20)]
dist = stats.beta
x = np.linspace(0, 1, 100)

for idx, N in enumerate(trials):
    if idx == 0:
        plt.subplot(4, 3, 2)
    else:
        plt.subplot(4, 3, idx+3)
    y = data[idx]
    for (a_prior, b_prior), c in zip(beta_params, ('b', 'r', 'g')):
        p_theta_given_y = dist.pdf(x, a_prior+y, b_prior+N-y)
        plt.fill_between(x, 0, p_theta_given_y, color=c, alpha=0.6)

    plt.axvline(theta_real, ymax=0.3, color='k')
    plt.plot(0,0,label="{:d} experiments\n{:d} heads".format(N,y),alpha=0)
    plt.xlim(0,1)
    plt.ylim(0,12)
    plt.xlabel(r'$\theta$')
    plt.legend()
    plt.gca().axes.get_yaxis().set_visible(False)
plt.tight_layout()
plt.savefig("posterior.png")
\end{lstlisting}

\begin{figure}[htb]
\center{\includegraphics[width=10cm]  {posterior.png}}
\caption{\label{beta_distribution}  posterior}
\end{figure}

\subsubsection{Model notation and visualization}
A common notation to succinctly represent probabilistic is as follows:
\begin{align*}
\noindent
  \checkmark \theta \sim Beta(\alpha, \beta)\\
\noindent
  \checkmark y \sim Bin(n=1,p=\theta)
\end{align*}